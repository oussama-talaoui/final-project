---
title: "DATA PROCESSES FINAL PROJECT (GROUP 3)"
#author: "Group-3"
#date: "18/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(corrplot)
library(ppcor)
library(dplyr)
library(GGally)
library(tseries)
library(purrr)
library(tidyr)
library(readxl)
library(recipes)
library(mlr)
library(mlbench)
library(e1071)
library(kknn)
library(rpart)
library(kernlab)
library(nnet)
library(unbalanced)
library(DiscriMiner)
library(FSelectorRcpp)
library(praznik)
library(randomForest)
library(ada)
library(RWeka)
library(gridExtra)
library (rpart.plot)

data <- read_excel("data/default-of-credit-card-clients.xls")
data <- data[,-1] #droping the ID features

#rename features, to work better with them
data <- data %>%
  rename (BAL = LIMIT_BAL, RPSEP = PAY_0, RPAGO = PAY_2, 
          RPJUL = PAY_3, RPJUN = PAY_4, RPMAY = PAY_5, 
          RPABR = PAY_6, BILLSEP = BILL_AMT1, BILLAGO = BILL_AMT2,
          BILLJUL = BILL_AMT3, BILLJUN = BILL_AMT4, BILLMAY = BILL_AMT5,
          BILLABR = BILL_AMT6, PREPAYSEP = PAY_AMT1, PREPAYAGO = PAY_AMT2,
          PREPAYJUL = PAY_AMT3, PREPAYJUN = PAY_AMT4, PREPAYMAY = PAY_AMT5,
          PREPAYABR = PAY_AMT6, DEFAULT = `default payment next month`)
```

### Group Members

| Name                      | Github username           |
|---------------------------|---------------------------|
| ANGULO MONTES LUIS EDUARDO| LuisEduardoAngulo         |
| BORRERO GARCIA FRANCISCO  | Macvayne                  |
| TAHIRI ALAOUI OUSSAMA     | oussama-talaoui           |

### Abstract

The aim of this project was to work with a "credit card default" dataset to classify if a person would fall or not in credit default considering demographic and banking characteristics.

For each model that we realized to answer our question, there were no significant differences between the models in terms of the performance measures. The first one was Decision Tree, the second one was Logistic and the last one was Neural Network.

After all the training process we decided to test one solution with unseen data, we choose the classification tree algorithm to do this; the output exhibited that the performance measure in this step was even better than the one we saw for the training set and that the algorithm pointed as the most relevant features the repayment status in September and the repayment status in April, thus, the possibility of falling in credit default will be associated with them.

### Introduction and Related Work

#### Motivation:

The motivation of this project was to ﬁnd a simple and an effective predictive model for the banks to determine if their customers could make the credit card payments on-time. The banks with the invent of credit card were more focused on the number of customers using their credit service but the drawback of them not being able to pay back the credit in time was an issue that soon followed, a system was in need to effectively decide the credit limit to be allowed to a person based on his previous credit history, concentrating on this issue we were motivated to consider various parameters such as sex, age , level of education along with customer details and the credit history of the past 6 months to give a reliable and effective prediction if a customer is able to pay the credit.

So, the main goal was to classify the instances of our dataset according to their demographic and banking features between two classes, to see if they belong or not to credit default.

#### Relevant Work:

  - SR Islam, W Eberle, SK Ghafoor. (2018). "Credit default mining using combined machine learning and heuristic approach" Proceedings of the 2018 International Conference on Data Science, 16-22.
  - Sharma, Sunakshi & Mehra, Vipul. (2018). Default Payment Analysis of Credit Card Clients. 10.13140/RG.2.2.31307.28967.
  - Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480.
  - https://www.researchgate.net/publication/326171439_Default_Payment_Analysis_of_Credit_Card_Clients
  - https://ieeexplore.ieee.org/document/8776802

#### Exploratory Data Analysis:

- About The Dataset:

For this work, we used the "Taiwan's credit card default clients" dataset. This one was taken from the UCI Machine Learning Repository and it is composed of 24 features and 30.000 instances.
URL where dataset is from: [Dataset](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset)

Basically, it contains some demographic and banking information from credit cardholders of a bank in Taiwan for 2005. The features are described as follow:


```{r, echo=FALSE, results='asis'}
library(knitr)
# head(data, n=10)
kable(data[1:5,], format = "markdown")
```

The dimension of the dataset, the structure and number if missing values:


```{r, echo=FALSE}
dim(data) #dimension of the data set
str(data) #structure of the data set
#assess if we have missing values
sum(is.na(data))
```

The dataset doesn't not have any missing values.


#### Graphics:


```{r, echo=FALSE}
#we realize that the values of some features where different to the stated in the repository so we must change that
#changing the values
data <- data%>%
  mutate(RPSEP = RPSEP + 1,
         RPAGO = RPAGO + 1,
         RPJUL = RPJUL + 1,
         RPJUN = RPJUN + 1,
         RPMAY = RPMAY + 1,
         RPABR = RPABR + 1)

#Dummy encoding
data <- data %>%
  mutate(EDUCATION = ifelse(EDUCATION >= 4 | EDUCATION == 0, 4, EDUCATION),
         MARRIAGE = ifelse(MARRIAGE == 0, 3, MARRIAGE))

data <- data %>%
  mutate(MUJER = ifelse (SEX == 2, 1, 0), #0 hombre & 1 mujer,
         PREGRADO = ifelse (EDUCATION == 2, 1, 0), 
         HSCHOOL = ifelse (EDUCATION == 3, 1, 0),
         POSGRADO = ifelse (EDUCATION == 1, 1, 0),
         SINGLE = ifelse (MARRIAGE == 2, 1, 0),
         MARRIED = ifelse (MARRIAGE == 1, 1, 0))

#recategorize RP feature
data <- data %>%
  mutate(RPSEP = ifelse(RPSEP == 0, -1, RPSEP),
         RPAGO = ifelse(RPAGO == 0, -1, RPAGO),
         RPJUL = ifelse(RPJUL == 0, -1, RPJUL),
         RPJUN = ifelse(RPJUN == 0, -1, RPJUN),
         RPMAY = ifelse(RPMAY == 0, -1, RPMAY),
         RPABR = ifelse(RPABR == 0, -1, RPABR))

#Turning into a categorical feature (factor)
data$DEFAULT <- factor(data$DEFAULT, levels = c(0,1))
data$EDUCATION <- factor(data$EDUCATION, levels = c(1:4))
```

Basic graphs to check the distribution of 3 features:


```{r, echo=FALSE}
#Graphics

#1.Basic graph to check the distribution 3 features
#Histograms

ggplot(data = data) +
  aes(x = BAL)+
  geom_histogram(bins = 30, color="cornsilk4", fill="darkblue") +
  labs(title = "Credit Balance - Histogram", x = "Amount of the given credit (NT$)", y = "Frequency") 

ggplot(data = data) +
  aes(x = BILLSEP)+
  geom_histogram(bins = 30, color="cornsilk4", fill="darkblue") +
  labs(title = "Amount of bill in September - Histogram", x = "Amount of bill statement in September (NT$)", y = "Frequency") 

ggplot(data = data) +
  aes(x = PREPAYSEP)+
  geom_histogram(bins = 30, color="cornsilk4", fill="darkblue") +
  labs(title = "Amount of previous payment in September - Histogram", x = "Amount of previous payment in September (NT$)", y = "Frequency") 
```

Most of the numerical type features present a positive skewness.

Let us go deeper to understand the behavior of the default payment regarding the balance in the account:


```{r, echo=FALSE}
#Most of the numerical type features presented positive skewness.

#2.Lets go deeper to understand the behavior of the default payment regarding the balance in the account

ggplot(data = data) +
  aes(x = DEFAULT, y = BAL)+
  geom_boxplot(color="cornsilk4", fill="darkblue",
               notch = TRUE,
               notchwidth = 0.8,
               outlier.colour="red",
               outlier.fill="red",
               outlier.size=3) +
  labs(title = "Credit Balance and Default payment - Boxplot", subtitle="Defaulter Info based on Credit Given", x = "Default payment", y = "Amount of the given credit (NT$)") +  
  theme(panel.background = element_rect(fill = "gray97"))

#it is seem that there is differences between the groups, the people with default tends to have a lower balance account. We must highligth the outlier
```

It seems that there is differences between the groups, the people with default tends to have a lower balance account. We must highlight the outlier.


```{r, echo=FALSE}
#3.Graph 3

ggplot(data = data) +
  aes(x = EDUCATION, fill = DEFAULT)+
  geom_bar() +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Default payment and level of education", x = "Education level", y = "Observation count") +
  scale_x_discrete(labels = c('Graduate School','University','High School', 'Others'))

#here we can see that mostly university graduated people are the ones with more defaults
```

We can see that mostly university graduated people are the ones with more defaults.


```{r, echo=FALSE}
#4.Graph 4

g1 <- ggplot(data = data) +
  aes(x = factor(RPSEP), fill = DEFAULT)+
  geom_bar() +
  coord_flip() +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Repayment status in September and default payment", y = "Observation count", x = "Repayment status in September") 

g2 <- ggplot(data = data) +
  aes(x = factor(RPAGO), fill = DEFAULT)+
  geom_bar() +
  coord_flip() +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Repayment status in August and default payment", y = "Observation count", x = "Repayment status in August") 

g3 <- ggplot(data = data) +
  aes(x = factor(RPJUL), fill = DEFAULT)+
  geom_bar() +
  coord_flip() +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Repayment status in July and default payment", y = "Observation count", x = "Repayment status in July") 

g4 <- ggplot(data = data) +
  aes(x = factor(RPJUN), fill = DEFAULT)+
  geom_bar() +
  coord_flip() +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Repayment status in June and default payment", y = "Observation count", x = "Repayment status in June") 

g5 <- ggplot(data = data) +
  aes(x = factor(RPMAY), fill = DEFAULT)+
  geom_bar() +
  coord_flip() +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Repayment status in May and default payment", y = "Observation count", x = "Repayment status in May") 

g6 <- ggplot(data = data) +
  aes(x = factor(RPABR), fill = DEFAULT)+
  geom_bar() +
  coord_flip() +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Repayment status in April and default payment", y = "Observation count", x = "Repayment status in April") 

grid.arrange(g1,g2,g3,g4,g5,g6)
```

We can see that most of the people with default have payments delay for one or three months.


```{r, echo=FALSE}

#Here we can see that most of the people with defaul have payments delay for one or three months

#5.Graph

ggplot(data) +
  aes(x=BAL, y=AGE, color=DEFAULT) + 
  geom_point(size=3) +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Credit Balance, age of the credit holder and default payment", y = "Age", x = "Amount of the given credit (NT$)") 

#here we can see that there is not a visible pattern between the age and balance amount, also it is no posible to establish a relation between this two and the default payment

data <- data[,-2:-4] #removing sex, education $ marital status features, these ones were the original, we are dropping these because we have a dummy features for each one
```

And finally, we can see that there is not a visible pattern between the age and balance amount, also it is no possible to establish a relation between this two and the default payment.

### Methods

The appropriate methods are employed to answer the question of interest, including:

- **Strength of relationships**:

All the work related to machine learning algorithms and data preparation was done using R software, basically, "mlr" package, and others.

As mentioned before, the dataset did not have any missing values, but the data wrangling and engineering was necessary because of the categorical features, as they did not appear as it was mentioned in the repository. Therefore, the first thing that we did was a dummy encoding for sex, education and marriage features, where the following levels were dropped in each one: men, other education, other marital status.

Furthermore, in the data exploration, we realized that the repayment status was not between the proper range (-1 to 9), it was slid one unit (going for -2 to 8); thus, we added one unit to the scale. Regarding this feature, we chose not to encode it as a dummy due to it being ordinal, so it was used as it is.

The proper explanatory data analysis was conducted, checking correlations, skewness, kurtosis, normality distribution and so on; we found issues in the dataset with these measures. For instance, most of the numerical type features presented positive skewness. Moreover, we checked for potential outliers, scaling the features in a range from 0 to a 1, and dropping the ones that showed values higher than 3 (Euclidean distance threshold). The dataset was reduced to 29,265 instances, so we dropped around 735 instances that were potential outliers.

- **Prediction**:

For the proper deployment of the algorithms, the dataset was cut in two parts. The first subset was the training set, corresponds to the 80 % of the observations in the original dataset, and it was used to train all the algorithms, also to predict and assess the generalization of the model. On other hand, the test set was only used to evaluate one model (classification tree), to establish how well the model would behave with unseen data.

Once the dataset was split, we proceeded to scale the features because some of them were in different units of measurement. To fulfill this task, we used the min-max scaling function. Afterward, a statistical summary was applied to check the results. We realized that for the machine learning algorithms deployment, we would have some trouble with predictions because of the classes of our label feature. To solve this unbalance data issue, we performed a data balance process to avoid that in our prediction, the model will be tempted to misclassify the minority class. In general, machine learning algorithms tends to follow the majority, so they will perform well with the most common class and not that good with the uncommon one.

After all the process described before, we applied eight machine learning algorithms on the training set following the "mlr" approach (create a task, create a learner, train, predict, resampling). We implemented the following algorithms: Classification Tree, Logistic Regression, and Neural Network). Basically, we trained each one of them, then obtained a prediction for the training set, observed their performance and finally we evaluated their generalization with a resampling approach.

It is important to remark that for resampling (assessed of the algorithms) we used repeated cross-validation, setting the parameters in 3 folds and 2 repetitions.

### Results

```{r, echo=FALSE, results='hide'}
bd <- read_excel("data/default-of-credit-card-clients.xls")

bd <- bd[,-1] #droping the ID features

#data wrangling

#rename features, to work better with them
bd <- bd %>%
  rename (BAL = LIMIT_BAL, RPSEP = PAY_0, RPAGO = PAY_2, 
          RPJUL = PAY_3, RPJUN = PAY_4, RPMAY = PAY_5, 
          RPABR = PAY_6, BILLSEP = BILL_AMT1, BILLAGO = BILL_AMT2,
          BILLJUL = BILL_AMT3, BILLJUN = BILL_AMT4, BILLMAY = BILL_AMT5,
          BILLABR = BILL_AMT6, PREPAYSEP = PAY_AMT1, PREPAYAGO = PAY_AMT2,
          PREPAYJUL = PAY_AMT3, PREPAYJUN = PAY_AMT4, PREPAYMAY = PAY_AMT5,
          PREPAYABR = PAY_AMT6, DEFAULT = `default payment next month`)

#we realize that the values of some features where different to the stated in the repository so we must change that
#changing the values
bd <- bd%>%
  mutate(RPSEP = RPSEP + 1,
         RPAGO = RPAGO + 1,
         RPJUL = RPJUL + 1,
         RPJUN = RPJUN + 1,
         RPMAY = RPMAY + 1,
         RPABR = RPABR + 1)

#Dummy encoding
bd <- bd %>%
  mutate(EDUCATION = ifelse(EDUCATION >= 4 | EDUCATION == 0, 4, EDUCATION),
         MARRIAGE = ifelse(MARRIAGE == 0, 3, MARRIAGE))

bd <- bd %>%
  mutate(MUJER = ifelse (SEX == 2, 1, 0), #0 hombre & 1 mujer,
         PREGRADO = ifelse (EDUCATION == 2, 1, 0), 
         HSCHOOL = ifelse (EDUCATION == 3, 1, 0),
         POSGRADO = ifelse (EDUCATION == 1, 1, 0),
         SINGLE = ifelse (MARRIAGE == 2, 1, 0),
         MARRIED = ifelse (MARRIAGE == 1, 1, 0))

#recategorize RP feature
bd <- bd %>%
  mutate(RPSEP = ifelse(RPSEP == 0, -1, RPSEP),
         RPAGO = ifelse(RPAGO == 0, -1, RPAGO),
         RPJUL = ifelse(RPJUL == 0, -1, RPJUL),
         RPJUN = ifelse(RPJUN == 0, -1, RPJUN),
         RPMAY = ifelse(RPMAY == 0, -1, RPMAY),
         RPABR = ifelse(RPABR == 0, -1, RPABR))

bd <- bd[,-2:-4] #removing sex, education $ marital status features, these ones were the original, we are dropping these because we have a dummy features for each one

#Outliers
#Using the euclidean distances (if its higher than 3 is a unsual case)
bd <- bd%>%
  mutate(out_BAL = abs(scale(bd$BAL)),
         out_SEP = abs(scale(bd$BILLSEP)),
         out_AGO = abs(scale(bd$BILLAGO)),
         out_JUL = abs(scale(bd$BILLJUL)),
         out_JUN = abs(scale(bd$BILLJUN)),
         out_MAY = abs(scale(bd$BILLMAY)),
         out_ABR = abs(scale(bd$BILLABR)))

#removing the potencial outlier
bd <- bd%>%
  filter(out_BAL < 3 | out_SEP < 3 | out_AGO < 3 | out_JUL < 3 |
           out_JUN < 3, out_MAY < 3, out_ABR <3)

#removing the variables created for check the outlier
table(bd$DEFAULT)
bd <- bd[,-28:-34]

```

The summary of the data we used for training:

```{r, echo=FALSE, results='hide'}
#Split the dataset

bd$DEFAULT <- factor(bd$DEFAULT, levels = c(0,1))
set.seed(100)
index_1 <- sample(1:nrow(bd), round(nrow(bd) * 0.8))
train <- bd[index_1, ]
test  <- bd[-index_1, ]
```

```{r, echo=FALSE}
summary(train)
```

```{r, echo=FALSE, results='hide'}
#Normalize
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

train <- replace(train, 1:20,(apply(train[,1:20],2,normalize)))
test <- replace(test, 1:20,(apply(test[,1:20],2,normalize)))

#Balance the dataset
Y <- train[,21]
X <- train[,-21]
balance_train <- ubSMOTE(X = X, Y = Y$DEFAULT, perc.over = 100, perc.under = 300, k=3)
btrain <- as.data.frame(cbind(X = balance_train$X, DEFAULT = balance_train$Y))
table(btrain$DEFAULT)/nrow(btrain)

YT <- test[,21]
XT <- test[,-21]
balance_test <- ubSMOTE(X = XT, Y = as.factor(YT$DEFAULT), perc.over = 100, perc.under = 300, k=3)
btest <- as.data.frame(cbind(X = balance_test$X, DEFAULT = balance_test$Y))
table(btest$DEFAULT)/nrow(btest)

#task
btrain$DEFAULT <- factor(btrain$DEFAULT, levels = c(0,1))
clasificacion.task <- makeClassifTask(id = "task", data = btrain, target = "DEFAULT", positive = "1")
clasificacion.task
getTaskFeatureNames(clasificacion.task)
```

- **Decision Tree**:

As its name suggests, this algorithm follows a tree structure to predict a class, where each tree has its root node (a relevant feature to predict the class), leaf nodes (following features) and branches (output of the test). Works by partitioning the feature space into a number of smaller scenarios with similar response values using a set of splitting rules. The segmentation process is typically carried out using only one explanatory variable at a time.

Train:
```{r, echo=FALSE, results='hide'}
##DECISION TREE----

#Learner
getParamSet("classif.rpart")
learner.dt <- makeLearner("classif.rpart", 
                         predict.type = "response")
```

```{r, echo=FALSE}
#Train
mod.dt <- mlr::train(learner.dt, clasificacion.task)
getLearnerModel(mod.dt)

```

Prediction:
```{r, echo=FALSE}
#Predict
predict.dt <- predict(mod.dt, task = clasificacion.task)
head(as.data.frame(predict.dt))
```

```{r, echo=FALSE}
calculateConfusionMatrix(predict.dt)
predict.test.dt <- predict(mod.dt, newdata = btest)#TEST PREDICTION
```

Performance:
```{r, echo=FALSE, results='hide'}
#Performance
listMeasures(clasificacion.task)
performance(predict.dt, measures = list(acc, mmce, kappa))
```

```{r, echo=FALSE}
performance(predict.test.dt, measures = list(acc, mmce, kappa))#PERFORMANCE TEST

```

Resampling:
```{r, echo=FALSE}

#Resampling
RCV.dt <- repcv(learner.dt, clasificacion.task, folds = 3, reps = 2, 
             measures = list(acc, mmce, kappa), stratify = TRUE)
RCV.dt$aggr

```

Here is a graph of the generated tree:

```{r, echo=FALSE, results='hide'}

#GRAPH OF THE TREE
rpart.plot(mod.dt$learner.model,  box.palette="RdBu", shadow.col="gray", nn=TRUE)

```
For our data, the algorithm applied for building the tree used some of the available features, but the most relevant ones were X.RPSEP and X.RPABR. Thus, according to this algorithm, the possibility of falling in credit default will be associated with the repayment status in September and the repayment status in April.

- **Logistic**:

According to I-Chen & Chen-hiu “logistic regression is used for binary classification; problems that are confined to two classes, basically, a logistic regression model specifies that an appropriate function of the fitted probability of the event is a linear function of the observed values of the available explanatory variables”.

```{r, echo=FALSE, results='hide'}
##LOGISTIC----

#learner
getParamSet("classif.logreg")
learner.lr <- makeLearner("classif.logreg",
                         predict.type = "response")
```

Train:
```{r, echo=FALSE, results='hide'}
#Train
mod.lr <- mlr::train (learner.lr, clasificacion.task)
getLearnerModel(mod.lr)
```

```{r, echo=FALSE}
summary(mod.lr$learner.model)#OUTPUT

```

Prediction:
```{r, echo=FALSE,}

#Prediction
predict.lr <- predict(mod.lr, clasificacion.task)
calculateConfusionMatrix(predict.lr)

```

Performance:
```{r, echo=FALSE,}

#Performance
performance(predict.lr, measures = list(acc, mmce, kappa))

```

Resampling:
```{r, echo=FALSE,}

#Resampling
RCV.lr <- repcv(learner.lr, clasificacion.task, folds = 3, reps = 2, 
                measures = list(acc, mmce, kappa), stratify = TRUE)
RCV.lr$aggr

```
For the logistic regression output we are going to focus our attention on the mean misclassification error (MMCE), this performance measure evaluates the number of the wrong classifications from the total input of the training set (in this case).  The objective in a machine learning project is to minimize it; in our logistic model the mmce was 28 %, that means that this algorithm misclassified 28 % of the instances according to the observed class.

Going a little bit deeper on the output of the logistic regression we realized that some features were not statistically significant to the model (according to their p-value) some of them were: marital status, BILLJUN, BILLJUL, RPJUN and others. Meanwhile, features like RPSEP, BILLAGO, BILLJUL and others, increase the possibility of falling in credit default.

- **Neural Network**:

ANN are inspired by the way the human brain works, particularly, on the biological neuron structure. The approach holds all its power in a neuron, which is the central unit and the one in charge of the computations; thus, it receives features as inputs and then operates a function of the weighted sum of these plus the bias to generate an output. Essentially, the prediction of the class it's done by a learning process that is based on examples.

Learner:
```{r, echo=FALSE, results='hide'}
##Neural Network----
#learner
getParamSet("classif.nnet")
learner.nn <- makeLearner("classif.nnet", 
                          predict.type = "response")
```

```{r, echo=FALSE}
learner.nn$par.set

```

Train:
```{r, echo=FALSE,}

#Train
mod.nn <- mlr::train(learner.nn, clasificacion.task)
getLearnerModel(mod.nn)

```

Prediction:
```{r, echo=FALSE}

#Predict
predict.nn <- predict(mod.nn, task = clasificacion.task)
head(as.data.frame(predict.nn))
```

```{r, echo=FALSE, results='hide'}
calculateConfusionMatrix(predict.nn)
predict.test.nn <- predict(mod.nn, newdata = btest)

```

With the Neural Network method, we were capable of predicting that a credit holder would fall in default with an acceptable chance of misclassification. According to the performance measures, the model misclassificate an observation in 26 % of the opportunities.

Performance:
```{r, echo=FALSE,}

#Performance
performance(predict.nn, measures = list(acc, mmce, kappa))
```

```{r, echo=FALSE, results='hide'}
performance(predict.test.nn, measures = list(acc, mmce, kappa))

```

Resampling:
```{r, echo=FALSE, results='hide'}

#Resampling
RCV.nn <- repcv(learner.nn, clasificacion.task, folds = 3, reps = 2, 
             measures = list(acc, mmce, kappa), stratify = TRUE)

```

```{r, echo=FALSE}
RCV.nn$aggr

```

- **Benchmark**:

A comparison between the 3 models to judge, at the same time, their performance. The main idea with this was to compare and rank the methods to establish which one helps us to classify better in the training set and to determine which would generalize better using new data.

```{r, echo=FALSE, results='hide'}
##Benchmark----

#to compare between models
lrns <- list(learner.dt, learner.lr, learner.nn)
rdesc <- makeResampleDesc("RepCV", folds = 3, reps = 2) #Choose the resampling strategy
bmr <- benchmark(lrns, clasificacion.task, rdesc, measures = list(acc, mmce, kappa))
```

```{r, echo=FALSE}
getBMRPerformances(bmr, as.df = TRUE)
getBMRAggrPerformances(bmr, as.df = TRUE)

```

### Discussion and Future Work

- Real world implications:

This work can help in ﬁguring out what can be the distinctive features of a person who is been classiﬁed as a defaulter in the banking system. This can help the banks to make the informed decisions, for instance what all features need to be included in a person in order to issue a credit card or judge the eligibility or repaying capacity of a person.

- Future research:

The future research for this project can include using advanced neural network and support vector machine as an alternative data mining methodologies to actually see if the true positive rate for predicting the defaulters is increasing along with the accuracy of the model. We could also apply the binding of the ages to segment the customers according to their age range.

### Code Quality

We assured that our code is well commented, structured and organized across different files, as we included a Script.R file in the repository with all the code used for this project. As well as, some code snippet that we included to generate the analysis and visualizations in this report.
